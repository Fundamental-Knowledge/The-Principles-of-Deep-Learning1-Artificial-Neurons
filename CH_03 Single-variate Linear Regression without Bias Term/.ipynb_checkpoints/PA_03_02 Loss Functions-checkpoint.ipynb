{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter.3 Single-variate Linear Regression without Bias Term\n",
    "\n",
    "## Programming Assignment.3-02 Loss Functions\n",
    "\n",
    "이번 PA 03-02에서는 하나의 data sample에 대해 loss function이 어떻게 형성되는지 살펴보고, 이 data sample을 이용하여 predictor를 학습시켜봅니다.\n",
    "\n",
    "또한 여러개의 data sample들에 대해 각각 loss function이 어떻게 달라지는지 확인하고, 각 data sample들을 이용하여 predictor를 학습시키면 어떤 변화가 생기는지 살펴봅니다.\n",
    "\n",
    "이때 각 data sample들은 PA 03-01과 마찬가지로 $y = 3x$에서부터 만들어집니다.\n",
    "\n",
    "PA 03-02는 다음과 같은 Step들로 이루어집니다.\n",
    "<ul>\n",
    "    <li> Step.1 Loss Function for One Sample </li>\n",
    "    <li> Step.2 Learning with One Sample </li>\n",
    "    <li> Step.3 Cost Functions for Different Data Samples </li>\n",
    "    <li> Step.4 Learning with Different Data Samples </li>\n",
    "    <li> Step.5 Learning with Dataset </li>\n",
    "    <li> Step.6 Random Shuffling </li>\n",
    "</ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Step.1 Loss Function for One Sample\n",
    "\n",
    "먼저 data sample $(x^{(1)},y^{(1)}) = (1,4)$ 하나에 대한 loss function를 그려봅니다.  \n",
    "이 data sample에 대한 loss는 다음과 같이 구할 수 있습니다.\n",
    "\n",
    "$$\n",
    "\\mathcal{L}^{(1)} = (y^{(1)} - \\hat{y}^{(1)})^{2} = (y^{(1)} - \\theta x^{(1)})^{2}\n",
    "$$\n",
    "\n",
    "***\n",
    "**Programming**\n",
    "\n",
    "임의의 $\\theta$에 대한 loss function을 그리세요.  \n",
    "이때 $\\theta$는 target theta$(\\theta^{*})$는 3이므로 0부터 6까지 100개의 점을 이용하고, 이 $\\theta$에 따른 loss를 그립니다.\n",
    "\n",
    "(Hint.1) np.linspace( ): 임의의 $\\theta$를 설정  \n",
    "(Hint.2) np.power( ): $y - \\hat{y}$의 제곱을 구하기 위한 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Start Your Code(Data Sample) #####\n",
    "x1 = \n",
    "y1 = \n",
    "##### End Your Code(Data Sample) #####\n",
    "\n",
    "##### Start Your Code(Loss Function) #####\n",
    "th_range = \n",
    "loss_funct = \n",
    "##### End Your Code(Loss Function) #####\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (8,8))\n",
    "ax.plot(th_range, loss_funct)\n",
    "ax.tick_params(axis = 'both', labelsize = 20)\n",
    "ax.set_xlabel(r'$\\theta$', fontsize = 20)\n",
    "ax.set_ylabel(r'$\\mathcal{L}$', fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "<img src='./imgs/3_02_01.png' width = 400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 loss function에서 loss가 최소가 되는 $\\theta$가 target function의 weight와 같음을 확인하세요.  \n",
    "따라서 data sample을 가장 잘 표현하는 $\\theta$는 loss function을 최소화시키는 3이 됩니다.\n",
    "\n",
    "***\n",
    "**Programming**\n",
    "\n",
    "앞으로의 코드 중복을 방지하기 위하여 위의 loss function을 그리는 코드를 함수로 만들어줍니다.  \n",
    "이때, loss function을 그려줄 axes도 argument로 받아주도록 합니다.\n",
    "\n",
    "(Hint.1) 함수 내부의 코드는 위의 코드에서 argument만 조정해주면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss_funct(x, y, ax):\n",
    "    ##### Start Your Code(Loss Function Functionalization) #####\n",
    "    \n",
    "    ##### End Your Code(Loss Function Functionalization) #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Step.2 Learning with One Sample\n",
    "\n",
    "위의 x1, y1를 이용하여 predictor를 학습시켜봅니다.  \n",
    "data sample이 하나이므로 epoch보단 iteration이라는 표현이 적절하므로 PA 03-01에서의 epochs을 iterations으로 고쳐줍니다.  \n",
    "\n",
    "***\n",
    "**Programming**  \n",
    "이 학습을 위한 코드는 PA 03-01와 변수명 외에 달라지는 것은 없습니다.  \n",
    "이때 initial theta는 0.1, learning rate은 0.01, total iteration은 500으로 설정하세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Start Your Code(Learning Preparation) #####\n",
    "th = \n",
    "lr = \n",
    "iterations = \n",
    "##### End Your Code(Learning Preparation) #####\n",
    "\n",
    "th_list = []\n",
    "loss_list = []\n",
    "\n",
    "for iteration in range(iterations):\n",
    "    ##### Start Your Code(Loss Calculation) #####\n",
    "    pred = \n",
    "    loss = \n",
    "    ##### End Your Code(Loss Calculation) #####\n",
    "\n",
    "    th_list.append(th)\n",
    "    loss_list.append(loss)\n",
    "    \n",
    "    ##### Start Your Code(Gradient Descent Method) #####\n",
    "    th = \n",
    "    ##### End Your Code(Gradient Descent Method) #####\n",
    "    \n",
    "fig, ax = plt.subplots(2, 1, figsize = (20,10))\n",
    "ax[0].plot(th_list)\n",
    "ax[1].plot(loss_list)\n",
    "ax[0].set_title(r'$\\theta$', fontsize = 30)\n",
    "ax[1].set_title(r'$\\mathcal{L}$', fontsize = 30)\n",
    "for ax_idx in range(2):\n",
    "    ax[ax_idx].tick_params(axis = 'both', labelsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "<img src='./imgs/3_02_02.png' width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Programming**  \n",
    "위의 코드도 중복사용을 피하기 위해 함수로 만들어줍니다.\n",
    "\n",
    "trainer함수는 total iteration, learning rate의 정보와 data sample의 x,y값을 받아 th_list, loss_list를 return해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(iterations, lr, x, y):\n",
    "    ##### Start Your Code(Trainer Function Functionalization) #####\n",
    "    \n",
    "    ##### Start Your Code(Trainer Function Functionalization) #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Programming**  \n",
    "위에서 만든 2개의 함수를 이용하여 학습 코드를 단순화해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### Start Your Code(Data Sample) #####\n",
    "x1 = \n",
    "y1 = \n",
    "##### End Your Code(Data Sample) #####\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "\n",
    "##### Start Your Code(Loss Function Visualization) #####\n",
    "\n",
    "##### End Your Code(Loss Function Visualization) #####\n",
    "\n",
    "\n",
    "##### Start Your Code(Training) #####\n",
    "\n",
    "##### End Your Code(Training) #####\n",
    "\n",
    "ax.scatter(th_list, loss_list, color = 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "<img src='./imgs/3_02_03.png' width = 400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Step.3 Cost Functions for Different Data Samples\n",
    "\n",
    "위의 함수 2개로 학습을 확인한 코드를 이용하여 다른 data sample에 대해서도 학습을 시켜봅니다.\n",
    "\n",
    "***\n",
    "**Programming**  \n",
    "이번에는 동일한 target function $y = 3x$에서 만들어진 $(x^{(1)},y^{(1)}) = (2,6)$에 대해 학습을 진행해봅니다.  \n",
    "학습을 진행한 후, $(x^{(1)},y^{(1)}) = (1,3)$을 이용한 학습과의 차이점을 분석해보길 바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Start Your Code(Data Sample) #####\n",
    "x2 = \n",
    "y2 = \n",
    "##### End Your Code(Data Sample) #####\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "\n",
    "##### Start Your Code(Loss Function Visualization) #####\n",
    "\n",
    "##### End Your Code(Loss Function Visualization) #####\n",
    "\n",
    "\n",
    "##### Start Your Code(Training) #####\n",
    "\n",
    "##### End Your Code(Training) #####\n",
    "\n",
    "ax.scatter(th_list, loss_list, color = 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "<img src='./imgs/3_02_04.png' width = 400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Step.4 Learning with Different Data Samples\n",
    "\n",
    "위의 내용을 바탕으로 3개의 data sample들 \n",
    "$$ \\{(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), (x^{(3)},y^{(3)})\\} = \\{(0.5,1.5), (1,3), (2,6)\\} $$\n",
    "에 대해 학습이 진행되는 모습을 비교해봅니다.\n",
    "\n",
    "***\n",
    "**Programming**  \n",
    "아래의 코드는 같은 axes에 3개의 data sample들에 대해 $\\theta$가 update되는 모습을 비교하기 위한 코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array([0.5, 1, 2])\n",
    "y_data = 3*x_data\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "\n",
    "for x, y in zip(x_data, y_data):\n",
    "    ##### Start Your Code(Loss Funtion and Training) #####\n",
    "    \n",
    "    \n",
    "    ##### Start Your Code(Loss Funtion and Training) #####\n",
    "    ax.scatter(th_list, loss_list, s = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "<img src='./imgs/3_02_05.png' width = 400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Step.5 Learning with Dataset\n",
    "\n",
    "실제 딥러닝에서는 하나의 data sample만을 이용해 학습을 하지 않고, dataset 안에 들어있는 다양한 data sample들을 이용하여 학습합니다.  \n",
    "따라서 이번에는 dataset에 들어있는 data sample들 하나씩을 이용하여 $\\theta$를 학습시키는 방법을 알아봅니다.\n",
    "\n",
    "***\n",
    "**Programming**  \n",
    "미리 만들어놓은 x_data, y_data에 들어있는 x,y값을 이용하여 위해 built-in function zip()을 이용합니다.  \n",
    "이를 통해 하나의 data sample을 뽑은 뒤 gradient descent method를 이용하여 $\\theta$를 학습시킵니다.\n",
    "\n",
    "(Hint.1) 위에서 만든 함수를 이용하는 것이 아닌 PA_03_01와 마찬가지로 loss를 구한 뒤, gradient descent method를 이용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_data = np.array([0.5, 1, 2])\n",
    "y_data = 3*x_data\n",
    "\n",
    "##### Start Your Code(Learning Preparation) #####\n",
    "th = \n",
    "lr = \n",
    "epochs = \n",
    "##### End Your Code(Learning Preparation) #####\n",
    "\n",
    "th_list = []\n",
    "loss_list = []\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    for x,y in zip(x_data, y_data):\n",
    "        ##### Start Your Code(Loss Calculation) #####\n",
    "        pred = \n",
    "        loss = \n",
    "        ##### End Your Code(Loss Calculation) #####\n",
    "        \n",
    "        th_list.append(th)\n",
    "        loss_list.append(loss)\n",
    "        \n",
    "        ##### Start Your Code(Gradient Descent Method) #####\n",
    "        th = \n",
    "        ##### End Your Code(Gradient Descent Method) #####\n",
    "        \n",
    "fig, ax = plt.subplots(2, 1, figsize = (20,10))\n",
    "ax[0].plot(th_list, marker = 'o', markersize = 15)\n",
    "ax[1].plot(loss_list, marker = 'o', markersize = 15)\n",
    "ax[0].set_title(r'$\\theta$', fontsize = 30)\n",
    "ax[1].set_title(r'$\\mathcal{L}$', fontsize = 30)\n",
    "for ax_idx in range(2):\n",
    "    ax[ax_idx].tick_params(axis = 'both', labelsize = 20)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "<img src='./imgs/3_02_06.png' width = 600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Step.6 Random Shuffling\n",
    "\n",
    "실제 딥러닝 프로젝트에서는 dataset 전체를 이용하여 학습을 시킨 뒤, data sample의 순서를 바꿔줍니다.  \n",
    "이를 통해 $\\theta$에 대한 규칙적인 update를 막을 수 있습니다.\n",
    "\n",
    "***\n",
    "**Programming**  \n",
    "위에 작성한 학습 코드를 그대로 이용하되, epoch마다 random shuffle을 이용하여 학습을 진행하고,  \n",
    "random shuffle의 유무에 따라 학습의 경향이 어떻게 바뀌는지 분석해보세요.\n",
    "\n",
    "(Hint.1) np.random.shuffle()  \n",
    "(Hint.2) np.random.shuffle()은 row-wise로 진행되기 때문에 x_data, y_data를 하나의 변수에 담아주는 것이 편합니다.  \n",
    "(Hint.3) np.hstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = np.array([0.5, 1, 2]).reshape(-1,1)\n",
    "y_data = 3*x_data\n",
    "##### Start Your Code(np.hstack) #####\n",
    "data = \n",
    "##### End Your Code(np.hstack) #####\n",
    "\n",
    "##### Start Your Code(Learning Preparation) #####\n",
    "th = \n",
    "lr = \n",
    "epochs = \n",
    "##### End Your Code(Learning Preparation) #####\n",
    "\n",
    "th_list = []\n",
    "loss_list = []\n",
    "    \n",
    "for epoch in range(epochs):\n",
    "    np.random.shuffle(data)\n",
    "    for x,y in data:\n",
    "        ##### Start Your Code(Loss Calculation) #####\n",
    "        pred = \n",
    "        loss = \n",
    "        ##### End Your Code(Loss Calculation) #####\n",
    "        \n",
    "        th_list.append(th)\n",
    "        loss_list.append(loss)\n",
    "        \n",
    "        ##### Start Your Code(Gradient Descent Method) #####\n",
    "        th = \n",
    "        ##### End Your Code(Gradient Descent Method) #####\n",
    "        \n",
    "fig, ax = plt.subplots(2, 1, figsize = (20,10))\n",
    "ax[0].plot(th_list, marker = 'o', markersize = 15)\n",
    "ax[1].plot(loss_list, marker = 'o', markersize = 15)\n",
    "ax[0].set_title(r'$\\theta$', fontsize = 30)\n",
    "ax[1].set_title(r'$\\mathcal{L}$', fontsize = 30)\n",
    "for ax_idx in range(2):\n",
    "    ax[ax_idx].tick_params(axis = 'both', labelsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "<img src='./imgs/3_02_07.png' width = 600>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
