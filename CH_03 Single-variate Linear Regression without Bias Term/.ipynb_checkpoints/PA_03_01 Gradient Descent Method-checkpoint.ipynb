{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter.3 Single-variate Linear Regression without Bias Term\n",
    "\n",
    "## Programming Assignment.3-01 Gradient Descent Method\n",
    "\n",
    "이번 PA 03-01에서는 기본적인 NumPy와 Matplotlib의 사용법을 익히고, Gradient Descent Method을 이용하여 predictor를 학습시켜 봅니다.\n",
    "\n",
    "PA 03-01은 다음과 같은 단계로 이루어집니다.\n",
    "\n",
    "<ul>\n",
    "    <li> Step.1 Dataset Generation </li>    \n",
    "    NumPy를 이용하여 PA 03-01에 필요한 dataset을 만듭니다.\n",
    "    <li> Step.2 Gradient Descent Method </li>\n",
    "    GDM을 이용하여 predictor를 학습시킵니다.\n",
    "    <li> Step.3 Predictor Visualization </li>\n",
    "    predictor가 학습되는 모습을 시각화하여 제대로 학습이 일어나는지 확인합니다.        \n",
    "</ul>\n",
    "\n",
    "먼저 다음 셀을 실행시켜 필요한 library들을 import하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Step.1 Dataset Generation\n",
    "\n",
    "### Target Function\n",
    "linear regression model을 학습시키기 위한 dataset을 먼저 만들어봅니다. dataset은\n",
    "\n",
    "$$y = 3x$$\n",
    "\n",
    "에서부터 만들어지며, 따라서 target function은 $y = 3x$입니다.\n",
    "\n",
    "***\n",
    "**Programming**\n",
    "\n",
    "먼저 x값을 기준으로 -3부터 3까지 target function을 그려봅니다.\n",
    "\n",
    "(Hint.1) 직선을 그리기 위해선 시작점, 끝점 2개가 필요합니다.\n",
    "\n",
    "(Hint.2) np.linspace()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Start Your Code(Targer Function) #####\n",
    "t_x = \n",
    "t_y = \n",
    "##### End Your Code(Targer Function) #####\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (7,7))\n",
    "ax.plot(t_x, t_y, linestyle = ':')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "<img src=\"./imgs/3_01_01.png\" width = 300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Samples\n",
    "\n",
    "signle-variate linear regression의 dataset은 1개의 x값과 그에 따른 1개의 y값으로 이루어집니다.\n",
    "\n",
    "그리고 target function과 같은 함수에서부터 만들어지며 Gaussian noise가 추가됩니다.\n",
    "\n",
    "***\n",
    "**Programming**\n",
    "\n",
    "PA 03-01에서는 100개의 data sample들을 standard normal distribution에서부터 만들고, noise도 마찬가지로 standard normal distribution에서부터 만들어진 Gaussian noise를 추가합니다. 이때 Gaussian noise에 0.5를 곱해줍니다.\n",
    "\n",
    "(Hint.1) np.random.normal( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Start Your Code(Dataset Generation) #####\n",
    "n_sample = \n",
    "x_data = \n",
    "y_data = \n",
    "##### End Your Code(Dataset Generation) #####\n",
    "\n",
    "# Target Function Visualization\n",
    "fig, ax = plt.subplots(figsize = (7,7))\n",
    "ax.plot(t_x, t_y, linestyle = ':')\n",
    "\n",
    "# Dataset Visualization\n",
    "ax.scatter(x_data, y_data, color = 'r')\n",
    "ax.tick_params(axis = 'both', labelsize = 20)\n",
    "ax.set_title(\"Dataset\", fontsize = 30)\n",
    "ax.set_xlabel(\"x data\", fontsize = 20)\n",
    "ax.set_ylabel(\"y data\", fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "<img src=\"./imgs/3_01_02.png\" width = 400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Step.2 Gradient Descent Method\n",
    "\n",
    "위의 dataset에 따라 predictor는\n",
    "$$ \\hat{y} = \\theta x$$\n",
    "로 설정할 수 있습니다.\n",
    "\n",
    "그리고 i번째 data sample에 대한 loss($\\mathcal{L}^{(i)}$)는 \n",
    "$$ \\mathcal{L}^{(i)} = (y^{(i)} - \\hat{y}^{(i)})^{2} = (y^{(i)} - \\theta x^{(i)})^{2}$$\n",
    "로 정의됩니다.\n",
    "\n",
    "따라서 $\\theta$에 대한 $\\mathcal{L}^{(i)}$의 partial derivative는\n",
    "$$ \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial \\theta} = -2x^{(i)}(y^{(i)} - \\hat{y}^{(i)})$$\n",
    "입니다.\n",
    "\n",
    "마지막으로 이를 이용한 Gradient descent method는\n",
    "\n",
    "$$ \\theta := \\theta - \\alpha \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial \\theta}$$\n",
    "\n",
    "$$ = \\theta + 2\\alpha x^{(i)}(y^{(i)} - \\hat{y}^{(i)})$$\n",
    "입니다.\n",
    "\n",
    "***\n",
    "**Programming**\n",
    "\n",
    "initial theta(th)는 0.1, learning rate(lr)는 0.001, 총 epoch(epochs)는 50으로 설정하고 gradient descent method를 이용하여 optimal theta에 대한 approximation을 진행하세요.\n",
    "\n",
    "(Hint.1) $\\theta$의 변화와 loss를 추적하기 위하여 빈 리스트 th_list, loss_list를 각각 만들어줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Start Your Code(Learning Preparation) #####\n",
    "th = \n",
    "lr = \n",
    "epochs = \n",
    "##### End Your Code(Learning Preparation) #####\n",
    "\n",
    "th_list = []\n",
    "loss_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for data_idx, (x, y) in enumerate(zip(x_data, y_data)):\n",
    "        ##### Start Your Code(Loss Calculation) #####\n",
    "        pred = \n",
    "        loss = \n",
    "        ##### End Your Code(Loss Calculation) #####\n",
    "\n",
    "        th_list.append(th)\n",
    "        loss_list.append(loss)\n",
    "        \n",
    "        ##### Start Your Code(Gradient Descent Method) #####\n",
    "        th = \n",
    "        ##### End Your Code(Gradient Descent Method) #####\n",
    "        \n",
    "fig, ax = plt.subplots(2, 1, figsize = (20,10))\n",
    "ax[0].plot(th_list)\n",
    "ax[1].plot(loss_list)\n",
    "ax[0].set_title(r'$\\theta$', fontsize = 30)\n",
    "ax[1].set_title(r'$\\mathcal{L}$', fontsize = 30)\n",
    "for ax_idx in range(2):\n",
    "    ax[ax_idx].tick_params(axis = 'both', labelsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "<img src=\"./imgs/3_01_03.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Step.3 Predictor Visualization\n",
    "\n",
    "다음 셀을 실행하여 초기 predictor $\\hat{y} = 0.1x$가 학습을 거쳐 dataset을 잘 표현하는 predictor로 학습되는지 확인하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (7,7))\n",
    "ax.scatter(x_data, y_data, color = 'r')\n",
    "ax.tick_params(axis = 'both', labelsize = 20)\n",
    "ax.set_title(\"Dataset\", fontsize = 30)\n",
    "ax.set_xlabel(\"x data\", fontsize = 20)\n",
    "ax.set_ylabel(\"y data\", fontsize = 20)\n",
    "\n",
    "x_range = np.linspace(-3, 3, 2)\n",
    "cmap = cm.get_cmap('rainbow', lut = len(th_list))\n",
    "for th_idx, th in enumerate(th_list):\n",
    "    predictor = th*x_range\n",
    "    ax.plot(x_range, predictor,\n",
    "            color = cmap(th_idx),\n",
    "            alpha = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**\n",
    "<img src=\"./imgs/3_01_04.png\" width = 400>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
