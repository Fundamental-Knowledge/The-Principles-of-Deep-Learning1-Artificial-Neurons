{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter.3 Single-variate Linear Regression without Bias Term\n",
    "\n",
    "## Programming Assignment.3-06 SVLR without Bias Term for One Sample\n",
    "\n",
    "PA 3-06에서는 PA 3-05에서 만든 dataset generator를 통해 dataset을 만들고,  \n",
    "PA 3-04에서 만든 basic building node들을 이용하여 bias term이 없는 single-variate linear model을 학습시킵니다.\n",
    "\n",
    "PA 3-06은 다음의 단계들을 통해 SVLR without bias term model의 학습원리를 확인합니다.\n",
    "\n",
    "<ul>\n",
    "    <li> Step.1 Dataset Preparation </li>\n",
    "    <li> Step.2 Model/Loss Implementation </li>\n",
    "    <li> Step.3 Learning </li>\n",
    "    <li> Step.4 Predictor Visualization </li>\n",
    "</ul>\n",
    "\n",
    "먼저 다음의 셀을 실행시켜 필요한 library들과 basic nodes, dataset generator를 import하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import os\n",
    "import sys\n",
    "utils_path = os.path.dirname(os.path.abspath(__name__)) + '/../utils/'\n",
    "if utils_path not in sys.path:    \n",
    "    sys.path.append(utils_path)\n",
    "\n",
    "import basic_nodes as nodes\n",
    "from LR_dataset_generator import LR_dataset_generator\n",
    "    \n",
    "plt.style.use('seaborn')\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Step.1 Dataset Preparation\n",
    "\n",
    "SVLR without bias term을 위한 dataset은\n",
    "$$y = ax$$\n",
    "의 식에서부터 만들어집니다.\n",
    "\n",
    "그리고 LR_dataset_generator는 coefficient를 지정해주지 않으면 weight는 1, bias는 0으로 설정해주기 때문에  \n",
    "feature_dim만 1로 설정해주면\n",
    "$$y = x$$\n",
    "에서부터 dataset을 만듭니다.\n",
    "\n",
    "***\n",
    "**Programming**  \n",
    "$$y = x$$\n",
    "LR_dataset_generator를 이용하여 dataset을 만드세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "##### Start Your Code(Dataset Preparation) #####\n",
    "data_gen = \n",
    "dataset = \n",
    "##### End Your Code(Dataset Preparation) #####\n",
    "\n",
    "x_data, y_data = dataset[:,:-1], dataset[:,-1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (7,7))\n",
    "ax.scatter(x_data[:,1], y_data)\n",
    "ax.tick_params(axis = 'both', labelsize = 20)\n",
    "ax.set_xlabel(\"X Data\", fontsize = 20)\n",
    "ax.set_ylabel(\"Y Data\", fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**  \n",
    "<img src='./imgs/3_06_02.png' width = 400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Step.2 Model/Loss Implementation\n",
    "\n",
    "Step.2에서는 학습에 필요한 model과 loss function을 basic building node들을 이용하여 구현합니다.  \n",
    "강의에서 사용된 model과 loss는 다음과 같습니다.\n",
    "<img src='./imgs/3_06_01.png' width = 600>\n",
    "\n",
    "따라서 model을 implementation하기 위해선 mul_node가 필요하고,  \n",
    "loss를 implementation하기 위해선 minus_node와 square_node가 필요합니다.\n",
    "\n",
    "***\n",
    "**Programming**  \n",
    "SVLR without bias term model과 square error loss를 구현하는데 필요한 basic building node를 instantiation하세요.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Start Your Code(Model Implementation) #####\n",
    "node1 = \n",
    "##### End Your Code(Model Implementation) #####\n",
    "\n",
    "\n",
    "##### Start Your Code(Loss Implementation) #####\n",
    "node2 = \n",
    "node3 = \n",
    "##### End Your Code(Loss Implementation) #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Step.3 Learning\n",
    "\n",
    "Step.3에서는 Step.1에서 만든 dataset과 Step.2에서 만든 node들을 이용하여 학습을 진행합니다.  \n",
    "deep learning에서의 학습은 model이 가지고 있는 learnable parameter인 $\\theta$를 대상으로 하고,  \n",
    "1. forward propagation에서 loss를 계산한 뒤,  \n",
    "2. chain rule을 이용한 backpropagation을 통해 loss에 대한 $\\theta$의 partial derivative를 구합니다.  \n",
    "3. 그리고 gradient descent method를 이용해 learnable parameter $\\theta$를 iterative하게 학습시킵니다.\n",
    "\n",
    "***\n",
    "**Programming**  \n",
    "SVLR without bias term model을 학습시키세요.\n",
    "\n",
    "이를 위하여 먼저 학습에 필요한 parameter를 설정해줍니다. 설정해줄 parameter들은 다음과 같습니다.\n",
    "- initial theta = 0.1\n",
    "- learning rate = 0.01\n",
    "- total epoch = 1\n",
    "\n",
    "그리고 실제 model을 학습시켜봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Start Your Code(Learning Preparation) #####\n",
    "th = \n",
    "lr = \n",
    "epochs = \n",
    "##### End Your Code(Learning Preparation) #####\n",
    "\n",
    "th_list = []\n",
    "loss_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for data_sample in dataset:\n",
    "        ##### Start Your Code(x, y Allocation) #####\n",
    "        x, y = \n",
    "        ##### End Your Code(x, y Allocation) #####\n",
    "        \n",
    "        \n",
    "        ##### Start Your Code(Forward Propagation) #####\n",
    "        z1 = \n",
    "        z2 = \n",
    "        l = \n",
    "        ##### End Your Code(Forward Propagation) #####\n",
    "        \n",
    "        \n",
    "        ##### Start Your Code(Backpropagation) #####\n",
    "        dz2 = \n",
    "        dy, dz1 = \n",
    "        dth, dx = \n",
    "        ##### End Your Code(Backpropagation) #####\n",
    "        \n",
    "        th_list.append(th)\n",
    "        loss_list.append(l)\n",
    "        \n",
    "        ##### Start Your Code(Gradient Descent Method) #####\n",
    "        th = \n",
    "        ##### End Your Code(Gradient Descent Method) #####\n",
    "        \n",
    "fig, ax = plt.subplots(2, 1, figsize = (15,8))\n",
    "fig.subplots_adjust(hspace = 0.3)\n",
    "ax[0].plot(th_list)\n",
    "ax[1].plot(loss_list)\n",
    "ax[0].tick_params(axis = 'both', labelsize = 20)\n",
    "ax[1].tick_params(axis = 'both', labelsize = 20)\n",
    "ax[0].set_title(r'$\\theta$', fontsize = 30)\n",
    "ax[1].set_title(r'$\\mathcal{L}$', fontsize = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**  \n",
    "<img src='./imgs/3_06_03.png' width = 700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Step.4 Predictor Visualization\n",
    "\n",
    "다음의 셀을 실행시켜 initial model이 target function에 가까워지도록 학습이 일어나는지 확인하세요.  \n",
    "그리고 $\\theta, \\mathcal{L}$의 변화와 predictor의 변화를 비교하며 학습의 원리를 분석해보세요.  \n",
    "\n",
    "**Further Works**   \n",
    "- predictor를 완벽히 학습시키기 위해서는 Step.3에서 어떤 변화를 줘야하는지 생각해보고, 수정 후 Step.3, Step.4를 다시 실행시켜보세요.\n",
    "- loss가 감소하면서 fluctuation이 심하게 일어나는 이유에 대하여 분석하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (15,10))\n",
    "left, bottom = 0.1, 0.1\n",
    "width, height = 0.3, 0.4\n",
    "spacing = 0.05\n",
    "\n",
    "ax_th = fig.add_axes([left, bottom + height + spacing, width,\n",
    "                        1-(2*bottom + height + spacing)])\n",
    "ax_loss = fig.add_axes([left, bottom, width, height])\n",
    "ax_pred = fig.add_axes([left + width + spacing, bottom,\n",
    "                       1 - (2*left + width + spacing), 2*height])\n",
    "\n",
    "ax_th.set_title(r'$\\theta$', fontsize = 30)\n",
    "ax_loss.set_title(r'$\\mathcal{L}$', fontsize = 30)\n",
    "ax_pred.set_title('Predictor', fontsize = 30)\n",
    "ax_th.plot(th_list)\n",
    "ax_loss.plot(loss_list)\n",
    "\n",
    "ax_pred.scatter(dataset[:,1], dataset[:,-1])\n",
    "cmap = cm.get_cmap('rainbow', lut = len(th_list))\n",
    "x_range = np.array([np.min(dataset[:,1]), np.max(dataset[:,1])])\n",
    "for th_idx, th in enumerate(th_list):\n",
    "    pred = th*x_range\n",
    "    \n",
    "    ax_pred.plot(x_range, pred, color = cmap(th_idx),\n",
    "                alpha = 0.3)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**  \n",
    "<img src='./imgs/3_06_04.png' width = 800>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Programming**  \n",
    "Step.3에서 작성한 training code를 추후 여러번 이용하기 때문에 함수화 해줍니다.\n",
    "\n",
    "다음 정의되는 trainer 함수의 input/output은 다음과 같습니다.\n",
    "- INPUT : dataset, initial theta, learning rate, total epochs\n",
    "- OUTPUT : th_list, loss_list\n",
    "\n",
    "이를 위해 Step.3에서 작성한 코드를 이용해 다음 함수를 채우세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(dataset, th, lr, epochs):\n",
    "    ##### Start Your Code(trainer Functionalization) #####\n",
    "    \n",
    "    ##### End Your Code(trainer Functionalization) #####\n",
    "    \n",
    "    fig = plt.figure(figsize = (15,10))\n",
    "    left, bottom = 0.1, 0.1\n",
    "    width, height = 0.3, 0.4\n",
    "    spacing = 0.05\n",
    "\n",
    "    ax_th = fig.add_axes([left, bottom + height + spacing, width,\n",
    "                            1-(2*bottom + height + spacing)])\n",
    "    ax_loss = fig.add_axes([left, bottom, width, height])\n",
    "    ax_pred = fig.add_axes([left + width + spacing, bottom,\n",
    "                           1 - (2*left + width + spacing), 2*height])\n",
    "\n",
    "    ax_th.set_title(r'$\\theta$', fontsize = 30)\n",
    "    ax_loss.set_title(r'$\\mathcal{L}$', fontsize = 30)\n",
    "    ax_pred.set_title('Predictor', fontsize = 30)\n",
    "    ax_th.plot(th_list)\n",
    "    ax_loss.plot(loss_list)\n",
    "\n",
    "    ax_pred.scatter(dataset[:,1], dataset[:,-1])\n",
    "    cmap = cm.get_cmap('rainbow', lut = len(th_list))\n",
    "    x_range = np.array([np.min(dataset[:,1]), np.max(dataset[:,1])])\n",
    "    for th_idx, th in enumerate(th_list):\n",
    "        pred = th*x_range\n",
    "\n",
    "        ax_pred.plot(x_range, pred, color = cmap(th_idx),\n",
    "                    alpha = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Programming**  \n",
    "\n",
    "위의 trainer 함수를 이용하여 다음의 조건에서의 학습을 진행하세요.  \n",
    "- initial theta = 0.1\n",
    "- learning rate = 0.008\n",
    "- epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Start Your Code(Learning Preparation) #####\n",
    "th = \n",
    "lr = \n",
    "epochs = \n",
    "##### End Your Code(Learning Preparation) #####\n",
    "\n",
    "\n",
    "##### Start Your Code(trainer Function) #####\n",
    "th_list, loss_list = \n",
    "##### End Your Code(trainer Function) #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**  \n",
    "<img src='./imgs/3_06_05.png' width = 800>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
