{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter.3 Single-variate Linear Regression without Bias Term\n",
    "\n",
    "## Programming Assignment.3-08 SVLR without BIas Term for Several Samples\n",
    "\n",
    "PA 3-08에서는 여러개의 data sample을 이용하여 $\\theta$를 학습시키는 방법을 구현합니다.  \n",
    "이때 PA 3-08에선 dataset에 포함된 모든 data sample들을 이용하는 방법인 Batch Gradient Descent Method를 다룹니다.\n",
    "\n",
    "이 과정을 학습하기 위해 PA 3-08은 다음의 단계로 이루어집니다.\n",
    "\n",
    "<ul>\n",
    "    <li> Step.1 Dataset Preparation </li>\n",
    "    <li> Step.2 Model/Cost Implementation </li>\n",
    "    <li> Step.3 Vector Size Test </li>\n",
    "    <li> Step.4 Learning </li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여러개의 data sample을 이용하여 $\\theta$를 학습시킬 때 loss가 아닌 cost를 이용합니다.  \n",
    "그리고 이 cost에 대한 $\\theta$의 partial derivative를 구하고,  \n",
    "이 partial derivative를 이용하여 gradient descent method를 통해 $\\theta$를 학습시킵니다.\n",
    "\n",
    "이 과정을 수식으로 표현하면 다음과 같습니다.  \n",
    "$$\\mathcal{L}^{(i)} = (y^{(i)} - \\hat{y}^{(i)})^{2}$$\n",
    "$$J = \\frac{1}{n}\\sum_{i=1}^{n} \\mathcal{L}^{(i)}$$\n",
    "$$\\frac{\\partial J}{\\partial \\theta} = \\frac{1}{n}\\sum_{i=1}^{n} \\frac{\\partial \\mathcal{L}^{(i)}}{\\theta}$$\n",
    "\n",
    "위의 과정을 진행하기 위해 먼저 다음 셀을 실행시켜 필요한 library와 module들을 import 해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import os\n",
    "import sys\n",
    "utils_path = os.path.dirname(os.path.abspath(__name__)) + '/../utils/'\n",
    "if utils_path not in sys.path:    \n",
    "    sys.path.append(utils_path)\n",
    "\n",
    "import basic_nodes as nodes\n",
    "from LR_dataset_generator import LR_dataset_generator\n",
    "    \n",
    "plt.style.use('seaborn')\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Step.1 Dataset Preparation\n",
    "\n",
    "여러개의 data sample을 이용하여 학습을 진행할 때 dataset에는 변화가 없습니다.  \n",
    "따라서 PA 3-06에서 사용한 dataset preparation 코드를 그대로 사용합니다.\n",
    "\n",
    "***\n",
    "**Programming**  \n",
    "SVLR without bias term model을 위한 dafault dataset을 만드세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Start Your Code(Dataset Preparation) #####\n",
    "data_gen = \n",
    "dataset = \n",
    "##### Start Your Code(Dataset Preparation) #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Step.2 Model/Cost Implementation\n",
    "\n",
    "Intro에서 설명한 연산을 basic building node들을 이용하여 표현하면 다음과 같습니다.\n",
    "\n",
    "<img src='./imgs/3_08_01.png' width = 600>\n",
    "따라서 위의 연산을 하기 위해선 model에는 변화가 없고 PA 3-06의 implementation부분에서 mean_node()가 추가됩니다.  \n",
    "\n",
    "***\n",
    "**Programming**  \n",
    "model과 cost를 연산하는 과정을 위한 node들을 instantiation하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Start Your Code(Model Implementation) #####\n",
    "node1 = \n",
    "##### End Your Code(Model Implementation) #####\n",
    "\n",
    "\n",
    "##### Start Your Code(Cost Implementation) #####\n",
    "node2 = \n",
    "node3 = \n",
    "node4 = \n",
    "##### End Your Code(Cost Implementation) #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Step.3 Vector Size Test\n",
    "\n",
    "Step.3에서는 Step.2에서 instantiation된 node들을 이용하여 forward/backward propagation이 될 때  \n",
    "node들의 input/output vector들의 dimension을 확인해봅니다.\n",
    "\n",
    "그리고 각 과정에서 강의에서 설명한 다음의 내용과 일치하는지 확인해보세요.  \n",
    "이때 NumPy의 특성을 이용하여 Jacobian matrix들은 vector form으로 바뀌어 연산됩니다.\n",
    "<img src='./imgs/3_08_02.png'>\n",
    "\n",
    "먼저 test_x, test_y들을 다음과 같이 100-dimensional vector로 만들고 th는 1로 설정해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = np.random.randn(100)\n",
    "test_y = np.random.randn(100)\n",
    "th = 1\n",
    "\n",
    "print(\"Shape of test_x : \", test_x.shape)\n",
    "print(\"Shape of test_y : \", test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**  \n",
    "Shape of test_x :  (100,)  \n",
    "Shape of test_y :  (100,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Programming**  \n",
    "위의 test_x, test_y를 이용하여 forward propagation을 구현하고  \n",
    "Z1, Z2, Z3, J의 shape을 확인합니다.  \n",
    "(Hint.1) shape이 ()이면 scalar를 의미합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Start Your Code(Forward Propagation) #####\n",
    "Z1 = \n",
    "Z2 = \n",
    "Z3 = \n",
    "J = \n",
    "##### End Your Code(Forward Propagation) #####\n",
    "\n",
    "print(\"Shape of Z1 : \", Z1.shape)\n",
    "print(\"Shape of Z2 : \", Z2.shape)\n",
    "print(\"Shape of Z3 : \", Z3.shape)\n",
    "print(\"Shape of J : \", J.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**  \n",
    "Shape of Z1 :  (100,)  \n",
    "Shape of Z2 :  (100,)  \n",
    "Shape of Z3 :  (100,)  \n",
    "Shape of J :  ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Programming**  \n",
    "다음으로 backpropagation을 구현하고  \n",
    "dZ3, dZ2, dZ1, dTh의 shape을 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Start Your Code(Backpropagation) #####\n",
    "dZ3 = \n",
    "dZ2 = \n",
    "dY, dZ1 = \n",
    "dTh, dX = \n",
    "##### End Your Code(Backpropagation) #####\n",
    "\n",
    "print(\"Shape of dZ3 : \", dZ3.shape)\n",
    "print(\"Shape of dZ2 : \", dZ2.shape)\n",
    "print(\"Shape of dZ1 : \", dZ1.shape)\n",
    "print(\"Shape of dTh : \", dTh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**  \n",
    "Shape of dZ3 :  (100,)  \n",
    "Shape of dZ2 :  (100,)  \n",
    "Shape of dZ1 :  (100,)  \n",
    "Shape of dTh :  (100,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dTh는 아직 vector이기 때문에 이들을 모두 합하여 실제 $\\theta$를 학습시키기 위한 dth를 구해야 합니다.\n",
    "\n",
    "***\n",
    "**Programming**  \n",
    "dTh의 원소들을 모두 합하여 dth를 구하세요.  \n",
    "(Hint.1) np.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Start Your Code(dth) #####\n",
    "dth = \n",
    "##### End Your Code(dth) #####\n",
    "\n",
    "print(\"Shape of dth : \", dth.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**  \n",
    "Shape of dth :  ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Step.4 Learning\n",
    "\n",
    "Step.3까지의 내용을 바탕으로 batch gradient descent method를 구현합니다.  \n",
    "이때 batch gradient descent method는 dataset을 모두 이용하기 때문에 epoch보단 iteration이라는 이름으로 학습조건을 설정해줍니다.\n",
    "\n",
    "PA 3-06에서부터 달라지는 점은 다음과 같습니다.\n",
    "- data sample을 뽑는 과정이 없어집니다.\n",
    "- dTh를 np.sum()을 이용하여 dth로 만들어준 뒤, gradient descent method를 적용합니다.\n",
    "\n",
    "***\n",
    "**Programming**  \n",
    "다음의 조건에서 model을 학습시키는 과정을 구현하세요.\n",
    "- initial theta = 0.1\n",
    "- learning rate = 0.01\n",
    "- total iteration = 300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Start Your Code(Learning Preparation) #####\n",
    "th = \n",
    "lr = \n",
    "iterations = \n",
    "##### End Your Code(Learning Preparation) #####\n",
    "\n",
    "th_list = []\n",
    "loss_list = []\n",
    "    \n",
    "for iteration in range(iterations):\n",
    "    ##### Start Your Code(Forward Propagation) #####\n",
    "    X, Y = \n",
    "    Z1 = \n",
    "    Z2 = \n",
    "    Z3 = \n",
    "    J = \n",
    "    ##### End Your Code(Forward Propagation) #####\n",
    "    \n",
    "    \n",
    "    ##### Start Your Code(Backpropagation) #####\n",
    "    dZ3 = \n",
    "    dZ2 = \n",
    "    dY, dZ1 = \n",
    "    dTh, dX = \n",
    "    ##### End Your Code(Backpropagation) #####\n",
    "    \n",
    "    th_list.append(th)\n",
    "    loss_list.append(J)\n",
    "    \n",
    "    ##### Start Your Code(Gradient Descent Method) #####\n",
    "    th = \n",
    "    ##### End Your Code(Gradient Descent Method) #####\n",
    "      \n",
    "fig, ax = plt.subplots(2, 1, figsize = (15,8))\n",
    "fig.subplots_adjust(hspace = 0.3)\n",
    "ax[0].plot(th_list)\n",
    "ax[1].plot(loss_list)\n",
    "ax[0].tick_params(axis = 'both', labelsize = 20)\n",
    "ax[1].tick_params(axis = 'both', labelsize = 20)\n",
    "ax[0].set_title(r'$\\theta$', fontsize = 30)\n",
    "ax[1].set_title(r'$\\mathcal{L}$', fontsize = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**  \n",
    "<img src='./imgs/3_08_03.png'>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
