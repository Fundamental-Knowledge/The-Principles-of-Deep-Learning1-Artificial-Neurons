{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter.4 Single-variate Linear Regression\n",
    "\n",
    "## Programming Assignment.4-01 Gradient Descent Method\n",
    "\n",
    "Chapter4에서 bias term이 추가되었기 때문에 dataset도 \n",
    "$$y = ax + b$$\n",
    "와 같은 식에서부터 만들어지며 이에 따라 model도\n",
    "$$\\hat{y} = \\theta_{1}x + \\theta_{0}$$\n",
    "가 됩니다.\n",
    "\n",
    "따라서 이번 PA 4-01에서는 다음의 단계들을 통해 $\\hat{y} = \\theta_{1}x + \\theta_{0}$에 대해 gradient descent method를 적용하여 $\\theta_{1}, \\theta_{0}$을 학습시킵니다.\n",
    "\n",
    "<ul>\n",
    "    <li> Step.1 Dataset Preparation </li>\n",
    "    <li> Step.2 One Iteration of GDM </li>\n",
    "    <li> Step.3 Gradient Descent Method </li>\n",
    "    <li> Step.4 Predictor Visualization </li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "utils_path = os.path.dirname(os.path.abspath(__name__)) + '/../utils/'\n",
    "if utils_path not in sys.path:    \n",
    "    sys.path.append(utils_path)\n",
    "\n",
    "from LR_dataset_generator import LR_dataset_generator\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Step.1 Dataset Preparation\n",
    "\n",
    "SVLR을 위한 dataset은\n",
    "$$y = ax + b$$\n",
    "에서부터 만들어지기 때문에 default dataset에서 coefficient를 customizing해줘야 합니다.\n",
    "\n",
    "다음은\n",
    "$$y = 2x + 1$$\n",
    "에서부터 200개의 data sample을 가지는 dataset을 만드는 코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 3)\n"
     ]
    }
   ],
   "source": [
    "n_sample = 200\n",
    "coefficient_list = [1, 2]\n",
    "\n",
    "data_gen = LR_dataset_generator(feature_dim = 1)\n",
    "data_gen.set_n_sample(n_sample)\n",
    "data_gen.set_coefficient(coefficient_list)\n",
    "dataset = data_gen.make_dataset()\n",
    "\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Programming**  \n",
    "위의 코드를 이용하여\n",
    "$$y = 5x + 2$$\n",
    "에서부터 1000개의 data sample을 가지는 dataset을 만드세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Start Your Code(Learning Preparation) #####\n",
    "n_sample = \n",
    "coefficient_list = \n",
    "\n",
    "\n",
    "dataset = data_gen.make_dataset()\n",
    "##### End Your Code(Learning Preparation) #####\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**  \n",
    "(300, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Step.2 One Iteration of GDM\n",
    "\n",
    "SVLR의 predictor는 다음과 같다.\n",
    "$$\n",
    "\\hat{y} = \\theta_{1}x + \\theta_{0}\n",
    "$$\n",
    "\n",
    "따라서 1개의 data sample에 대한 loss는\n",
    "\n",
    "$$ \\mathcal{L}^{(i)} = (y^{(i)} - \\hat{y}^{(i)})^{2} = (y^{(i)} - (\\theta_{1}x^{(i)} + \\theta_{0}))^{2}$$\n",
    "\n",
    "이므로 $\\mathcal{L}$은 $\\theta_{1}, \\theta_{0}$에 대한 함수이다.  \n",
    "이때 각각 $\\theta_{1}, \\theta_{0}$에 대한 partial derivative를 구하면\n",
    "\n",
    "$$\n",
    "\\frac {\\partial \\mathcal{L}^{(i)}} {\\partial \\theta_{1}}\n",
    "= -2x^{(i)}(y^{(i)} - \\hat{y}^{(i)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac {\\partial \\mathcal{L}^{(i)}} {\\partial \\theta_{0}}\n",
    "= -2(y^{(i)} - \\hat{y}^{(i)})\n",
    "$$\n",
    "\n",
    "이다. 위의 partial derivative를 이용하여 GDM을 적용하면 $\\theta_{1}, \\theta_{0}$은 다음과 같이 update됩니다.\n",
    "\n",
    "$$ \\theta_{1} := \\theta_{1} - \\alpha \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial \\theta} \n",
    "= \\theta_{1} + 2\\alpha x^{(i)}(y^{(i)} - \\hat{y}^{(i)})$$\n",
    "\n",
    "$$ \\theta_{0} := \\theta_{0} - \\alpha \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial \\theta} \n",
    "= \\theta_{0} + 2\\alpha(y^{(i)} - \\hat{y}^{(i)})$$\n",
    "\n",
    "***\n",
    "**Programming**  \n",
    "data sample $(x^{(i)}, y^{(i)}) = (1,12)$에 대하여 한 번의 iteration을 연산해보세요.  \n",
    "이때 학습 조건은 다음과 같습니다.\n",
    "- initial theta1, theta0 = 1, 1\n",
    "- learning rate = 0.01\n",
    "- x, y = 2, 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Start Your Code(Initial Setting) #####\n",
    "th1, th0 = \n",
    "lr = \n",
    "x = \n",
    "y = \n",
    "##### End Your Code(Initial Setting) #####\n",
    "print(\"Before Update:\", th1, th0)\n",
    "\n",
    "\n",
    "##### Start Your Code(Partial Derivative Calculation) #####\n",
    "pred = \n",
    "dth1 = \n",
    "dth0 = \n",
    "##### Start Your Code(Partial Derivative Calculation) #####\n",
    "\n",
    "\n",
    "##### Start Your Code(Gradient Descent Method) #####\n",
    "th1 = \n",
    "th0 = \n",
    "##### Start Your Code(Gradient Descent Method) #####\n",
    "print(\"After Update:\", th1, th0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**  \n",
    "Before Update: 1 1  \n",
    "After Update: 1.3599999999999999 1.18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Step.3 Gradient Descent Method\n",
    "\n",
    "Step.3에서는 dataset에 들어있는 data sample들을 이용하여 $\\theta_{1}, \\theta_{0}$를 학습시킵니다.\n",
    "\n",
    "***\n",
    "**Programming**  \n",
    "PA 3-01과 마찬가지로 for loop을 이용하여 data sample에 접근하고, 각 data sample에 대해 GDM을 적용하여 $\\theta_{1}, \\theta_{0}를 학습시키세요.$  \n",
    "이때 학습 조건은 다음과 같습니다.\n",
    "- $\\theta_{1}, \\theta_{0} = 0.1, 0.1$\n",
    "- learning rate = 0.01\n",
    "- epochs = 2\n",
    "\n",
    "학습이 끝나면 target function $y = 5x + 2$에 가까워지도록 $\\theta_{1}, \\theta_{0}$가 학습되는지 확인하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##### Start Your Code(Learning Preparation) #####\n",
    "th1, th0 = \n",
    "lr = \n",
    "epochs = \n",
    "##### End Your Code(Learning Preparation) #####\n",
    "\n",
    "th1_list, th0_list = [], []\n",
    "loss_list = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for data_sample in dataset:\n",
    "        x, y = data_sample[1], data_sample[-1]\n",
    "        \n",
    "        ##### Start Your Code(Forward Propagation) #####\n",
    "        pred = \n",
    "        loss = \n",
    "        ##### Start Your Code(Forward Propagation) #####\n",
    "        \n",
    "        th1_list.append(th1)\n",
    "        th0_list.append(th0)\n",
    "        loss_list.append(loss)\n",
    "        \n",
    "        ##### Start Your Code(Gradient Descent Method) #####\n",
    "        th1 = \n",
    "        th0 = \n",
    "        ##### Start Your Code(Gradient Descent Method) #####\n",
    "        \n",
    "        \n",
    "fig, ax = plt.subplots(2, 1, figsize = (20,10))\n",
    "ax[0].plot(th1_list, label = r'$\\theta_{1}$')\n",
    "ax[0].plot(th0_list, label = r'$\\theta_{0}$')\n",
    "ax[0].legend(loc = 'upper left', fontsize = 30)\n",
    "ax[1].plot(loss_list)\n",
    "ax[0].set_title(r'$\\theta$', fontsize = 30)\n",
    "ax[1].set_title(r'$\\mathcal{L}$', fontsize = 30)\n",
    "for ax_idx in range(2):\n",
    "    ax[ax_idx].tick_params(axis = 'both', labelsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**  \n",
    "<img src='./imgs/4_01_01.png' width = 700>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Step.4 Predictor Visualization\n",
    "\n",
    "다음 셀을 실행하여 초기 predictor $\\hat{y} = 0.1x + 0.1$가 학습을 거쳐 dataset을 잘 표현하는 predictor로 학습되는지 확인하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (7,7))\n",
    "ax.scatter(dataset[:,1], dataset[:,-1], color = 'r')\n",
    "ax.tick_params(axis = 'both', labelsize = 20)\n",
    "ax.set_title(\"Dataset\", fontsize = 30)\n",
    "ax.set_xlabel(\"x data\", fontsize = 20)\n",
    "ax.set_ylabel(\"y data\", fontsize = 20)\n",
    "\n",
    "x_range = np.linspace(-4, 4, 2)\n",
    "cmap = cm.get_cmap('rainbow', lut = len(th1_list))\n",
    "for th_idx, (th1,th0) in enumerate(zip(th1_list, th0_list)):\n",
    "    predictor = th1*x_range + th0\n",
    "    ax.plot(x_range, predictor,\n",
    "            color = cmap(th_idx),\n",
    "            alpha = 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**  \n",
    "<img src='./imgs/4_01_02.png' width = 400>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
