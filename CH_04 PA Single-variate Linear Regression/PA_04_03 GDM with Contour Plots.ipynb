{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter.4 Single-variate Linear Regression\n",
    "\n",
    "## Programming Assignment.4-03 GDM with Contour Plots\n",
    "\n",
    "PA 4-03에서는 PA 4-02에서 만들었던 loss function들을 이용해 GDM을 적용해봅니다.  \n",
    "이를 통해 최종적으로 각 data sample을 이용하여 $\\theta_{1}, \\theta_{0}$를 update할 때 어떤 모습으로 학습되는지 확인합니다.\n",
    "\n",
    "그리고 다음과 같은 단계들로 이루어집니다.\n",
    "\n",
    "<ul>\n",
    "    <li> Step.1 GDM with $\\mathcal{L} = \\theta_{1}^{2} + \\theta_{0}^{2}$ </li>\n",
    "    <li> Step.2 GDM with $\\mathcal{L} = a\\theta_{1}^{2} + \\theta_{0}^{2}$ </li>\n",
    "    <li> Step.3 GDM with $\\mathcal{L} = \\theta_{1}^{2} + \\theta_{0}^{2} + c\\theta_{1}\\theta_{0}$ </li>\n",
    "    <li> Step.4 GDM with $\\mathcal{L} = (y - (\\theta_{1}x + \\theta_{0}))^{2}$ </li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Step.1 GDM with $\\mathcal{L} = \\theta_{1}^{2} + \\theta_{0}^{2}$\n",
    "\n",
    "먼저 \n",
    "$$\\mathcal{L}(\\theta_{1}, \\theta_{0}) = \\theta_{1}^{2} + \\theta_{0}^{2}$$\n",
    "에 대해 GDM을 적용해봅니다.\n",
    "\n",
    "이때 $\\mathcal{L}$에 대한 $\\theta_{1}, \\theta_{0}$의 partial derivative는 다음과 같습니다.\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\theta_{1}} = 2\\theta_{1}, \\quad\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\theta_{0}} = 2\\theta_{0}$$\n",
    "\n",
    "따라서 $\\theta_{1}, \\theta_{0}$에 대한 GDM formula는 다음과 같습니다.\n",
    "\n",
    "$$\\theta_{1} := \\theta_{1} - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial \\theta_{1}}\n",
    "= \\theta_{1} - 2\\alpha \\theta_{1}$$\n",
    "$$\\theta_{0} := \\theta_{0} - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial \\theta_{0}}\n",
    "= \\theta_{0} - 2\\alpha \\theta_{0}$$\n",
    "\n",
    "다음은 예제 코드로 위의 loss function에 대해 GDM을 적용하는 코드입니다.  \n",
    "이때 학습 조건은 다음과 같습니다.\n",
    "- initial theta1, theta0 = -0.5, -1.5\n",
    "- learning rate = 0.05\n",
    "- total iteration = 30\n",
    "\n",
    "\n",
    "다음 셀을 실행시켜 loss의 모습을 살펴보세요.  \n",
    "참고로 %matplotlib qt는 magic command로 3d plot을 회전하면서 확인할 수 있도록 외부 창에 그래프를 보여줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Preparation\n",
    "th1, th0 = -0.5, -1.5\n",
    "lr = 0.05\n",
    "iterations = 30\n",
    "\n",
    "th1_list, th0_list = [th1], [th0]\n",
    "\n",
    "# Loss Function\n",
    "th1_range = np.linspace(-2, 2, 100)\n",
    "th0_range = np.linspace(-2, 2, 100)\n",
    "Th1, Th0 = np.meshgrid(th1_range, th0_range)\n",
    "\n",
    "loss = np.power(Th1,2) + np.power(Th0, 2)\n",
    "\n",
    "# Gradient Descent MEthod\n",
    "for iteration in range(iterations):\n",
    "    # Partial Derivatives\n",
    "    dth1 = 2*th1\n",
    "    dth0 = 2*th0\n",
    "    \n",
    "    # Parameter Update\n",
    "    th1 = th1 - lr*dth1\n",
    "    th0 = th0 - lr*dth0\n",
    "    \n",
    "    th1_list.append(th1)\n",
    "    th0_list.append(th0)\n",
    "    \n",
    "\n",
    "# th1, th0 Update Visualization\n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "levels = np.geomspace(np.min(loss) + 0.01, np.max(loss), 30)\n",
    "cmap = cm.get_cmap('Reds_r', lut = len(levels))\n",
    "ax.contour(Th1, Th0, loss, levels = levels, cmap = cmap)\n",
    "ax.scatter(th1_list, th0_list,\n",
    "           s = 100, c = 'b')\n",
    "ax.tick_params(axis = 'both', labelsize = 20)\n",
    "ax.set_xlabel(r'$\\theta_{1}$', fontsize = 20)\n",
    "ax.set_ylabel(r'$\\theta_{0}$', fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Step.2 Contour Plot of $\\mathcal{L} = a\\theta_{1}^{2} + \\theta_{0}^{2}$\n",
    "\n",
    "Step.2에서는 $\\mathcal{L} = a\\theta_{1}^{2} + \\theta_{0}^{2}$에 대한 GDM을 진행합니다.  \n",
    "\n",
    "\n",
    "이때 $\\mathcal{L}$에 대한 $\\theta_{1}, \\theta_{0}$의 partial derivative는 다음과 같습니다.\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\theta_{1}} = 2a \\theta_{1}, \\quad\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\theta_{0}} = 2\\theta_{0}$$\n",
    "\n",
    "따라서 $\\theta_{1}, \\theta_{0}$에 대한 GDM formula는 다음과 같습니다.\n",
    "\n",
    "$$\\theta_{1} := \\theta_{1} - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial \\theta_{1}}\n",
    "= \\theta_{1} - 2a \\alpha \\theta_{1}$$\n",
    "$$\\theta_{0} := \\theta_{0} - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial \\theta_{0}}\n",
    "= \\theta_{0} - 2\\alpha \\theta_{0}$$\n",
    "\n",
    "즉, gradient $(\\frac{\\partial \\mathcal{L}}{\\partial \\theta_{1}}, \\frac{\\partial \\mathcal{L}}{\\partial \\theta_{0}}$)의 방향이 a에 영향을 받는다는 것을 알 수 있다.\n",
    "\n",
    "\n",
    "***\n",
    "**Programming**  \n",
    "다음의 조건에서 loss function\n",
    "$$\\mathcal{L}(\\theta_{1}, \\theta_{0}) = a\\theta_{1}^{2} + \\theta_{0}^{2}$$\n",
    "에 대해 1보다 큰 다양한 a에 대해 GDM을 적용해보고, 학습의 경향성을 분석하세요.\n",
    "\n",
    "- initial theta1, theta0 = -0.5, -1.5\n",
    "- learning rate = 0.05\n",
    "- total iteration = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Start Your Code(Learning Preparation) #####\n",
    "a = \n",
    "\n",
    "th1, th0 = \n",
    "lr = \n",
    "iterations = \n",
    "##### Start Your Code(Learning Preparation) #####\n",
    "\n",
    "th1_list, th0_list = [th1], [th0]\n",
    "\n",
    "th1_range = np.linspace(-2, 2, 100)\n",
    "th0_range = np.linspace(-2, 2, 100)\n",
    "Th1, Th0 = np.meshgrid(th1_range, th0_range)\n",
    "\n",
    "##### Start Your Code(Loss Function) #####\n",
    "loss = \n",
    "##### End Your Code(Loss Function) #####\n",
    "\n",
    "\n",
    "for iteration in range(iterations):\n",
    "    ##### Start Your Code(Partial Derivatives) #####\n",
    "    dth1 = \n",
    "    dth0 = \n",
    "    ##### Start Your Code(Partial Derivatives) #####\n",
    "    \n",
    "    \n",
    "    ##### Start Your Code(Parameter Update) #####\n",
    "    th1 = \n",
    "    th0 = \n",
    "    ##### Start Your Code(Parameter Update) #####\n",
    "    \n",
    "    \n",
    "    th1_list.append(th1)\n",
    "    th0_list.append(th0)\n",
    "    \n",
    "    \n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "levels = np.geomspace(np.min(loss) + 0.01, np.max(loss), 30)\n",
    "cmap = cm.get_cmap('Reds_r', lut = len(levels))\n",
    "ax.contour(Th1, Th0, loss, levels = levels, cmap = cmap)\n",
    "ax.scatter(th1_list, th0_list,\n",
    "           s = 100, c = 'b')\n",
    "ax.tick_params(axis = 'both', labelsize = 20)\n",
    "ax.set_xlabel(r'$\\theta_{1}$', fontsize = 20)\n",
    "ax.set_ylabel(r'$\\theta_{0}$', fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**  \n",
    "<img src='./imgs/4_03_01.png' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Programming**  \n",
    "다음의 조건에서 loss function\n",
    "$$\\mathcal{L}(\\theta_{1}, \\theta_{0}) = a\\theta_{1}^{2} + \\theta_{0}^{2}$$\n",
    "에 대해 0보다 크고 1보다 작은 다양한 a에 대해 GDM을 적용해보고, 학습의 경향성을 분석하세요.\n",
    "\n",
    "- initial theta1, theta0 = -0.5, -1.5\n",
    "- learning rate = 0.05\n",
    "- total iteration = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Start Your Code(Learning Preparation) #####\n",
    "a = \n",
    "\n",
    "th1, th0 = \n",
    "lr = \n",
    "iterations = \n",
    "##### End Your Code(Learning Preparation) #####\n",
    "\n",
    "th1_list, th0_list = [th1], [th0]\n",
    "\n",
    "th1_range = np.linspace(-2, 2, 100)\n",
    "th0_range = np.linspace(-2, 2, 100)\n",
    "Th1, Th0 = np.meshgrid(th1_range, th0_range)\n",
    "\n",
    "##### Start Your Code(Loss Function) #####\n",
    "loss = \n",
    "##### End Your Code(Loss Function) #####\n",
    "\n",
    "for iteration in range(iterations):\n",
    "    ##### Start Your Code(Partial Derivatives) #####\n",
    "    dth1 = \n",
    "    dth0 = \n",
    "    ##### End Your Code(Partial Derivatives) #####\n",
    "    \n",
    "    ##### Start Your Code(Parameter Update) #####\n",
    "    th1 = \n",
    "    th0 = \n",
    "    ##### End Your Code(Parameter Update) #####\n",
    "    \n",
    "    \n",
    "    th1_list.append(th1)\n",
    "    th0_list.append(th0)\n",
    "    \n",
    "    \n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "levels = np.geomspace(np.min(loss) + 0.01, np.max(loss), 30)\n",
    "cmap = cm.get_cmap('Reds_r', lut = len(levels))\n",
    "ax.contour(Th1, Th0, loss, levels = levels, cmap = cmap)\n",
    "ax.scatter(th1_list, th0_list,\n",
    "           s = 100, c = 'b')\n",
    "ax.tick_params(axis = 'both', labelsize = 20)\n",
    "ax.set_xlabel(r'$\\theta_{1}$', fontsize = 20)\n",
    "ax.set_ylabel(r'$\\theta_{0}$', fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**  \n",
    "<img src='./imgs/4_03_02.png' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Step.3 Step.3 GDM with $\\mathcal{L} = \\theta_{1}^{2} + \\theta_{0}^{2} + c\\theta_{1}\\theta_{0}$\n",
    "\n",
    "Step.3에서는 $\\mathcal{L} = \\theta_{1}^{2} + \\theta_{0}^{2} + c\\theta_{1}\\theta_{0}$에 대해 GDM을 적용해봅니다.  \n",
    "\n",
    "이때 $\\mathcal{L}$에 대한 $\\theta_{1}, \\theta_{0}$의 partial derivative는 다음과 같습니다.\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\theta_{1}} = 2\\theta_{1} + c\\theta_{0}, \\quad\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\theta_{0}} = 2\\theta_{0} + c\\theta_{1}$$\n",
    "\n",
    "따라서 $\\theta_{1}, \\theta_{0}$에 대한 GDM formula는 다음과 같습니다.\n",
    "\n",
    "$$\\theta_{1} := \\theta_{1} - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial \\theta_{1}}\n",
    "= \\theta_{1} - \\alpha(2\\theta_{1} + c\\theta_{0})$$\n",
    "$$\\theta_{0} := \\theta_{0} - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial \\theta_{0}}\n",
    "= \\theta_{0} - \\alpha(2\\theta_{0} + c\\theta_{1})$$\n",
    "\n",
    "\n",
    "***\n",
    "**Programming**  \n",
    "다음의 loss function\n",
    "$$\\mathcal{L}(\\theta_{1}, \\theta_{0}) = \\theta_{1}^{2} + \\theta_{0}^{2} + c\\theta_{1}\\theta_{0}$$\n",
    "에 대해 다양한 c를 사용하여 GDM을 적용해보고, 학습의 경향성을 분석하세요.  \n",
    "이때 c는 -2부터 2까지의 값이어야합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Start Your Code(Learning Preparation) #####\n",
    "c = \n",
    "\n",
    "th1, th0 = \n",
    "lr = \n",
    "iterations = \n",
    "##### End Your Code(Learning Preparation) #####\n",
    "\n",
    "th1_list, th0_list = [th1], [th0]\n",
    "\n",
    "th1_range = np.linspace(-2, 2, 100)\n",
    "th0_range = np.linspace(-2, 2, 100)\n",
    "Th1, Th0 = np.meshgrid(th1_range, th0_range)\n",
    "\n",
    "##### Start Your Code(Loss Function) #####\n",
    "loss = \n",
    "##### End Your Code(Loss Function) #####\n",
    "\n",
    "for iteration in range(iterations):\n",
    "    ##### Start Your Code(Partial Derivatives) #####\n",
    "    dth1 = \n",
    "    dth0 = \n",
    "    ##### End Your Code(Partial Derivatives) #####\n",
    "    \n",
    "    \n",
    "    ##### Start Your Code(Parameter Update) #####\n",
    "    th1 = \n",
    "    th0 = \n",
    "    ##### End Your Code(Parameter Update) #####\n",
    "    \n",
    "    th1_list.append(th1)\n",
    "    th0_list.append(th0)\n",
    "    \n",
    "    \n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "levels = np.geomspace(np.min(loss) + 0.01, np.max(loss), 30)\n",
    "cmap = cm.get_cmap('Reds_r', lut = len(levels))\n",
    "ax.contour(Th1, Th0, loss, levels = levels, cmap = cmap)\n",
    "ax.scatter(th1_list, th0_list,\n",
    "           s = 100, c = 'b')\n",
    "ax.tick_params(axis = 'both', labelsize = 20)\n",
    "ax.set_xlabel(r'$\\theta_{1}$', fontsize = 20)\n",
    "ax.set_ylabel(r'$\\theta_{0}$', fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**  \n",
    "<img src='./imgs/4_03_03.png' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Step.4 Loss Functions in Reality\n",
    "\n",
    "Step.4에서는 실제 loss function\n",
    "$$\\mathcal{L}(\\theta_{1},\\theta_{0})=(y - (\\theta_{1} x + \\theta_{0}))^{2}$$\n",
    "에 GDM을 적용해봅니다. \n",
    "\n",
    "그리고 Step.4에서는 data sample에 따른 학습에 집중하기 위해 한 번의 학습에는 하나의 data sample만을 사용합니다.  \n",
    "\n",
    "이때 $\\mathcal{L}$에 대한 $\\theta_{1}, \\theta_{0}$의 partial derivative는 다음과 같습니다.\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\theta_{1}} = -2x(y -\\hat{y}), \\quad\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\theta_{0}} = -2(y -\\hat{y})$$\n",
    "\n",
    "따라서 $\\theta_{1}, \\theta_{0}$에 대한 GDM formula는 다음과 같습니다.\n",
    "\n",
    "$$\\theta_{1} := \\theta_{1} - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial \\theta_{1}}\n",
    "= \\theta_{1} +2\\alpha x(y -\\hat{y})$$\n",
    "$$\\theta_{0} := \\theta_{0} - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial \\theta_{0}}\n",
    "= \\theta_{0} + 2\\alpha (y -\\hat{y})$$\n",
    "\n",
    "\n",
    "***\n",
    "**Programming**  \n",
    "다음의 조건에서 $y = x + 3$에서 만들어진 data point $(x,y) = (2,5)$를 이용하여 GDM을 진행하세요.\n",
    "\n",
    "- initial theta1, theta0 = -0.5, -1.5\n",
    "- (x,y) = (2,5)\n",
    "- learning rate = 0.05\n",
    "- total iteration = 30\n",
    "\n",
    "**Further Work** GDM이 진행되는 동안 $\\theta_{1}, \\theta_{0}$가 target theta인 $\\theta_{1}^{*}=1, \\theta_{0}^{*}=3$에 가까워지는지 확인하시오."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##### Start Your Code(Learning Preparation) #####\n",
    "x = \n",
    "y = \n",
    "\n",
    "th1, th0 = \n",
    "lr = \n",
    "iterations = \n",
    "##### End Your Code(Learning Preparation) #####\n",
    "th1_list, th0_list = [th1], [th0]\n",
    "\n",
    "th1_range = np.linspace(-1, 3, 100)\n",
    "th0_range = np.linspace(1, 5, 100)\n",
    "Th1, Th0 = np.meshgrid(th1_range, th0_range)\n",
    "\n",
    "##### Start Your Code(Loss Function) #####\n",
    "loss = \n",
    "##### End Your Code(Loss Function) #####\n",
    "\n",
    "for iteration in range(iterations):\n",
    "    ##### Start Your Code(Prediction Calculation) #####\n",
    "    pred = \n",
    "    ##### Start Your Code(Prediction Calculation) #####\n",
    "    \n",
    "    \n",
    "    ##### Start Your Code(Partial Derivatives) #####\n",
    "    dth1 = \n",
    "    dth0 = \n",
    "    ##### End Your Code(Partial Derivatives) #####\n",
    "    \n",
    "    \n",
    "    ##### Start Your Code(Parameter Update) #####\n",
    "    th1 = \n",
    "    th0 = \n",
    "    ##### End Your Code(Parameter Update) #####\n",
    "    \n",
    "    th1_list.append(th1)\n",
    "    th0_list.append(th0)\n",
    "    \n",
    "    \n",
    "fig, ax = plt.subplots(figsize = (10,10))\n",
    "levels = np.geomspace(np.min(loss) + 0.01, np.max(loss), 30)\n",
    "cmap = cm.get_cmap('Reds_r', lut = len(levels))\n",
    "ax.contour(Th1, Th0, loss, levels = levels, cmap = cmap)\n",
    "ax.scatter(th1_list, th0_list,\n",
    "           s = 100, c = 'b')\n",
    "ax.tick_params(axis = 'both', labelsize = 20)\n",
    "ax.set_xlabel(r'$\\theta_{1}$', fontsize = 20)\n",
    "ax.set_ylabel(r'$\\theta_{0}$', fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**  \n",
    "<img src='./imgs/4_03_04.png' width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Programming**  \n",
    "$y = x + 3$에서 만들어진 data sample들을 포함하는 다음과 같은 dataset이 주어졌다고 가정합니다.\n",
    "$$\\mathcal{D} = \\{ (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), (x^{(3)}, y^{(3)}), (x^{(4)}, y^{(4)}), (x^{(5)}, y^{(5)}), (x^{(6)}, y^{(6)}) \\} =\n",
    "\\{ (-2,1), (-1,2), (0,3), (1,4), (2,5), (5,8) \\}\n",
    "$$\n",
    "\n",
    "이때 각 data sample들에 대한 GDM을 진행하고, 각 data sample들을 이용하여 $\\theta_{1}, \\theta_{0}$을 update시킬 때의 차이점을 분석하세요.\n",
    "\n",
    "**Further Work** 2개의 learnable parameter $\\theta_{1}, \\theta_{0}$을 학습시키기 위해 1개의 data sample로 가능한지, 불가능한지 설명하고 그 이유를 제시하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##### Start Your Code(Dataset Preparation) #####\n",
    "x_data = \n",
    "y_data = \n",
    "##### End Your Code(Dataset Preparation) #####\n",
    "\n",
    "\n",
    "##### Start Your Code(Learning Preparation) #####\n",
    "lr = \n",
    "iterations = \n",
    "##### End Your Code(Learning Preparation) #####\n",
    "\n",
    "th1_range = np.linspace(-1, 3, 100)\n",
    "th0_range = np.linspace(1, 5, 100)\n",
    "Th1, Th0 = np.meshgrid(th1_range, th0_range)\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize = (15,30))\n",
    "for ax_idx, (ax, x, y) in enumerate(zip(axes.flat, x_data, y_data)):\n",
    "    ##### Start Your Code(Theta Initialization) #####\n",
    "    th1, th0 = \n",
    "    th1_list, th0_list = \n",
    "    ##### End Your Code(Theta Initialization) #####\n",
    "    \n",
    "    ##### Start Your Code(Loss Function) #####\n",
    "    loss = \n",
    "    ##### End Your Code(Loss Function) #####\n",
    "\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        ##### Start Your Code(Prediction Calculation) #####\n",
    "        pred = \n",
    "        ##### End Your Code(Prediction Calculation) #####\n",
    "\n",
    "        \n",
    "        ##### Start Your Code(Partial Derivatives) #####\n",
    "        dth1 = \n",
    "        dth0 = \n",
    "        ##### End Your Code(Partial Derivatives) #####\n",
    "\n",
    "        \n",
    "        ##### Start Your Code(Parameter Update) #####\n",
    "        th1 = \n",
    "        th0 = \n",
    "        ##### End Your Code(Parameter Update) #####\n",
    "\n",
    "        \n",
    "        th1_list.append(th1)\n",
    "        th0_list.append(th0)\n",
    "    \n",
    "    levels = np.geomspace(np.min(loss) + 0.01, np.max(loss), 30)\n",
    "    cmap = cm.get_cmap('Reds_r', lut = len(levels))\n",
    "    ax.contour(Th1, Th0, loss, levels = levels, cmap = cmap)\n",
    "    \n",
    "    ax.scatter(th1_list, th0_list,\n",
    "               s = 100, c = 'b')\n",
    "    ax.tick_params(axis = 'both', labelsize = 20)\n",
    "    ax.set_xlabel(r'$\\theta_{1}$', fontsize = 20)\n",
    "    ax.set_ylabel(r'$\\theta_{0}$', fontsize = 20)\n",
    "    ax.set_title('(x,y) = (%d,%d)'%(x,y), fontsize = 20)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**  \n",
    "<img src='./imgs/4_03_05.png' width=400>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
